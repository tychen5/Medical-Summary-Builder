from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Optional

from langchain_core.documents import Document

from ..config import settings
from ..data_ingestion import DocumentConverter, PDFMedicalLoader, TemplateLoader
from ..logging_config import configure_logging
from ..preprocessing import DocumentChunker, MetadataRouter, PageRelevanceRanker
from ..schemas import MedicalSummary
from ..utils import ensure_directory
from ..vectorstore import VectorIndexManager
from ..llm import EmbeddingFactory, LLMClientFactory


@dataclass
class PipelineArtifacts:
    """Handles intermediate artefacts generated by the pipeline."""

    raw_pages: list[Document]
    chunk_paths: list[Path]
    vector_index_name: str


class MedicalSummaryPipeline:
    """High-level orchestration of the medical summary generation workflow."""

    def __init__(self) -> None:
        self.llm = LLMClientFactory.create()
        self.embedding_model = EmbeddingFactory.create()
        self.chunker = DocumentChunker(settings.chunk_size, settings.chunk_overlap)
        self.metadata_router = MetadataRouter(self.llm)
        self.page_ranker = PageRelevanceRanker(self.llm)
        self.vector_index_manager = VectorIndexManager(self.embedding_model)

    def setup(self) -> None:
        configure_logging()
        ensure_directory(settings.outputs_dir)
        ensure_directory(settings.reports_dir)
        ensure_directory(settings.diagnostics_dir)

    def ingest_source(self, pdf_path: Path | str) -> list[Document]:
        loader = PDFMedicalLoader(pdf_path)
        return loader.load()

    def convert_template(self, template_path: Path | str) -> Path:
        template = TemplateLoader(template_path).load()
        markdown_path = settings.cache_dir / "templates" / "medical_summary.md"
        ensure_directory(markdown_path.parent)
        markdown_path.write_text("\n".join(paragraph.text for paragraph in template.paragraphs), encoding="utf-8")
        return markdown_path

    def build_vector_index(self, documents: list[Document]) -> None:
        chunked_documents = self.chunker.split(documents)
        langchain_documents = [
            Document(page_content=chunk.text, metadata=chunk.metadata)
            for chunk in chunked_documents
        ]
        self.vector_index_manager.upsert(langchain_documents)

    def run(
        self,
        *,
        pdf_path: Path | str,
        template_path: Path | str,
        custom_instruction: Optional[str] = None,
    ) -> MedicalSummary:
        self.setup()
        documents = self.ingest_source(pdf_path)
        self.build_vector_index(documents)
        self.convert_template(template_path)
        # Further steps: metadata extraction, agent-driven table filling, report generation
        return MedicalSummary()
